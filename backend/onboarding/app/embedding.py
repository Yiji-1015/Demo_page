"""
ì„ë² ë”© ë° Qdrant ë²¡í„° DB ê´€ë¦¬ ëª¨ë“ˆ (ì¤‘ë³µ ë°©ì§€ ê¸°ëŠ¥ ê°•í™”)
"""

import os
import uuid
from typing import List, Dict, Any
from openai import OpenAI
from qdrant_client import QdrantClient, models
from qdrant_client.models import Distance, VectorParams
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

class EmbeddingManager:
    """ì„ë² ë”© ë° ë²¡í„° DB ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(
        self,
        upstage_api_key: str,
        qdrant_url: str,
        qdrant_api_key: str,
        collection_name: str = "confluence_docs"
    ):
        self.collection_name = collection_name

        # Upstage í´ë¼ì´ì–¸íŠ¸
        self.client_upstage = OpenAI(
            api_key=upstage_api_key,
            base_url="https://api.upstage.ai/v1"
        )

        # Qdrant í´ë¼ì´ì–¸íŠ¸
        self.client_qdrant = QdrantClient(
            url=qdrant_url,
            api_key=qdrant_api_key
        )

        # í…ìŠ¤íŠ¸ ë¶„í• ê¸°
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=3000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""]
        )

    def embedding(self, text: str) -> List[float]:
        response = self.client_upstage.embeddings.create(
            input=text,
            model="embedding-query"
        )
        return response.data[0].embedding

    def embedding_batch(self, texts: List[str]) -> List[List[float]]:
        if not texts:
            return []
        response = self.client_upstage.embeddings.create(
            input=texts,
            model="embedding-query"
        )
        return [data.embedding for data in response.data]

    def ensure_collection_exists(self):
        """Qdrant ì»¬ë ‰ì…˜ ë° ì¸ë±ìŠ¤ ë³´ì¥ (ì¤‘ìš”!)"""
        # 1. ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±
        if not self.client_qdrant.collection_exists(collection_name=self.collection_name):
            self.client_qdrant.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=4096, # Upstage Solar Embedding ì°¨ì› ìˆ˜
                    distance=Distance.COSINE
                ),
            )
            print(f"ğŸ“¦ Qdrant ì»¬ë ‰ì…˜ '{self.collection_name}' ì‹ ê·œ ìƒì„± ì™„ë£Œ")
        else:
            print(f"ğŸ“¦ Qdrant ì»¬ë ‰ì…˜ '{self.collection_name}'ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

        # 2. ğŸš¨ í•„ìˆ˜: ì»¬ë ‰ì…˜ì´ ìˆë“  ì—†ë“  ì¸ë±ìŠ¤ëŠ” ë°˜ë“œì‹œ í™•ì¸/ìƒì„±í•´ì•¼ í•¨
        try:
            self.client_qdrant.create_payload_index(
                collection_name=self.collection_name,
                field_name="page_id",
                field_schema=models.PayloadSchemaType.KEYWORD
            )
            print(f"ğŸ” page_id ì¸ë±ìŠ¤ ìƒì„±(ë˜ëŠ” í™•ì¸) ì™„ë£Œ")
        except Exception as e:
            # ì´ë¯¸ ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¬´ì‹œ (ì •ìƒ)
            pass

    def get_collection_info(self) -> Dict[str, Any]:
        try:
            collection_info = self.client_qdrant.get_collection(self.collection_name)
            return {
                "collection_name": self.collection_name,
                "points_count": collection_info.points_count,
                "status": collection_info.status,
            }
        except Exception as e:
            return {"error": str(e)}

    # âœ… [í•µì‹¬] ì´ë¯¸ ì €ì¥ëœ í˜ì´ì§€ì¸ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
    def is_page_indexed(self, page_id: str) -> bool:
        """
        íŠ¹ì • page_idê°€ ì´ë¯¸ DBì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        """
        try:
            # page_idë¥¼ ë°˜ë“œì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰ (ë§¤ìš° ì¤‘ìš”!)
            target_id = str(page_id)
            
            count_result = self.client_qdrant.count(
                collection_name=self.collection_name,
                count_filter=models.Filter(
                    must=[
                        models.FieldCondition(
                            key="page_id",
                            match=models.MatchValue(value=target_id)
                        )
                    ]
                )
            )
            
            # ë””ë²„ê¹…ìš© ë¡œê·¸ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
            # if count_result.count > 0:
            #     print(f" Â  ğŸ§ ID {target_id}ëŠ” ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
            
            return count_result.count > 0
            
        except Exception as e:
            print(f"âš ï¸ í˜ì´ì§€ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def create_documents(self, titles, page_ids, contents, base_url) -> List[Document]:
        documents = []
        for title, page_id, content in zip(titles, page_ids, contents):
            if not content or not content.strip():
                continue

            doc = Document(
                page_content=content,
                metadata={
                    "title": title,
                    "page_id": str(page_id),
                    "source": f"{base_url}/pages/{page_id}"
                }
            )
            documents.append(doc)
        return documents

    def chunk_documents(self, documents: List[Document]) -> List[Document]:
        split_docs = self.text_splitter.split_documents(documents)
        print(f"âœ‚ï¸ ì²­í‚¹ ì™„ë£Œ! ë¬¸ì„œ {len(documents)}ê°œ â†’ ì²­í¬ {len(split_docs)}ê°œ")
        return split_docs

    def delete_page_vectors(self, page_id: str):
        try:
            self.client_qdrant.delete(
                collection_name=self.collection_name,
                points_selector=models.FilterSelector(
                    filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key="page_id",
                                match=models.MatchValue(value=str(page_id))
                            )
                        ]
                    )
                )
            )
            # print(f" Â  ğŸ—‘ï¸ Â í˜ì´ì§€ {page_id} ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        except Exception as e:
            print(f" Â  âš ï¸ Â ì‚­ì œ ì¤‘ ê²½ê³ : {e}")

    def upsert_documents(self, page_id: str, documents: List[Document], batch_size: int = 32):
        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì—…ë°ì´íŠ¸ ì‹œ ì¤‘ë³µ ë°©ì§€)
        self.delete_page_vectors(page_id)

        total_docs = len(documents)
        print(f"ğŸ”„ [Upsert] Page {page_id}: {total_docs}ê°œ ì²­í¬ ì—…ë¡œë“œ ì‹œì‘")

        for i in range(0, total_docs, batch_size):
            batch_docs = documents[i : i + batch_size]
            batch_texts = [doc.page_content for doc in batch_docs]
            
            # ì„ë² ë”© ìƒì„±
            batch_vectors = self.embedding_batch(batch_texts)

            points = []
            for doc, vector in zip(batch_docs, batch_vectors):
                payload = doc.metadata.copy()
                payload["page_content"] = doc.page_content
                
                points.append(models.PointStruct(
                    id=str(uuid.uuid4()),
                    vector=vector,
                    payload=payload
                ))

            if points:
                self.client_qdrant.upload_points(
                    collection_name=self.collection_name,
                    points=points
                )
        print(f" Â  âœ… Page {page_id} ì—…ë¡œë“œ ì™„ë£Œ")

    # âœ… [í•µì‹¬ ë¡œì§] ì´ë¯¸ ìˆëŠ” ë¬¸ì„œëŠ” ê±´ë„ˆë›°ê³ , ì—†ëŠ” ê²ƒë§Œ ì²˜ë¦¬
    def upsert_multiple_pages(
        self,
        page_ids: List[str],
        titles: List[str],
        contents: List[str],
        base_url: str,
        batch_size: int = 32,
        force_update: bool = False # ê°•ì œ ì—…ë°ì´íŠ¸ ì˜µì…˜
    ):
        """
        ì—¬ëŸ¬ í˜ì´ì§€ë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤. (ê¸°ì¡´ì— ì—†ëŠ” í˜ì´ì§€ë§Œ ì²˜ë¦¬)
        """
        target_page_ids = []
        target_titles = []
        target_contents = []
        skipped_count = 0

        print(f"ğŸ§ ì¤‘ë³µ ë¬¸ì„œ í™•ì¸ ì¤‘... (ì´ {len(page_ids)}ê°œ)")

        for pid, title, content in zip(page_ids, titles, contents):
            # 1. ê°•ì œ ì—…ë°ì´íŠ¸ê°€ ì•„ë‹ˆê³ 
            # 2. ì´ë¯¸ DBì— ì¡´ì¬í•œë‹¤ë©´ -> ê±´ë„ˆë›°ê¸°
            if not force_update and self.is_page_indexed(pid):
                skipped_count += 1
                continue
            
            # 3. ì—†ëŠ” ë¬¸ì„œë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            target_page_ids.append(pid)
            target_titles.append(title)
            target_contents.append(content)

        if skipped_count > 0:
            print(f"â© {skipped_count}ê°œ ë¬¸ì„œëŠ” ì´ë¯¸ ìµœì‹  ìƒíƒœë¼ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤.")

        # 4. ì²˜ë¦¬í•  ë¬¸ì„œê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ë°”ë¡œ ì¢…ë£Œ
        if not target_page_ids:
            print("ğŸ‰ ëª¨ë“  ë¬¸ì„œê°€ ì´ë¯¸ ì„ë² ë”©ë˜ì–´ ìˆìŠµë‹ˆë‹¤! (ì‘ì—… ì—†ìŒ)")
            return

        print(f"ğŸš€ {len(target_page_ids)}ê°œ ì‹ ê·œ ë¬¸ì„œ ì„ë² ë”© ì‹œì‘...")

        # 5. ì—¬ê¸°ì„œë¶€í„°ëŠ” ì‹ ê·œ ë¬¸ì„œë§Œ ì²˜ë¦¬ë¨
        documents = self.create_documents(target_titles, target_page_ids, target_contents, base_url)
        split_docs = self.chunk_documents(documents)

        # ê·¸ë£¹í™”
        page_docs_map = {}
        for doc in split_docs:
            pid = doc.metadata["page_id"]
            if pid not in page_docs_map:
                page_docs_map[pid] = []
            page_docs_map[pid].append(doc)

        # ì—…ë¡œë“œ ì‹¤í–‰
        for pid, docs in page_docs_map.items():
            self.upsert_documents(pid, docs, batch_size)